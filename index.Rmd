---
title: "Practical Machine Learning Project"
author: "Adolfo Corrales"
date: "14th Mai 2018"
output: html_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

Using the **"Weight Lifting Exercises Dataset"** from the paper *Qualitative Activity Recognition of Weight Lifting Exercises* by Eduardo Velloso, Andreas Bulling, Hans Gellersen, Wallace Ugulino and Hugo Fuks, we performed an exercise of machine learning prediction. 

The goal of the project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did a particular exercise. The manner is a factor variable named "classe" in the training set. It has five possible outcomes or classes (from "A" to "E") that stand for the quality of execution for particular exercise. Class "A" is the correct way, the four other are deviations.

In any case, the idea is to use measures from the accelerometers (stored in a initial set of  160 variables) to predict the class in a test set of 20 different cases. We used different machine learning approaches and select the more accurate so as to be used in the prediction.

## Exploratory analysis

Training and test data are provided in the course project. After downloading the csv files, we proceed to read them in R and look into the structure of the data sets

```{r, echo=FALSE}
# set working directory
setwd("~/ADOLFO/COURSERA PRACTICAL MACHINE LEARNING/COURSE PROJECT")
#reading data
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

```{r}
dim(training); dim(testing)
```

The training set has more than 19,000 cases and 160 variables. The test set has only 20 cases and the same number of variables (160), however, the variable of interest "classe" is not present in the test set (a variable called "problem_id" is placed in the last column). This is important because accuracy cannot be measured in the test set and we have to rely in the accuracy measures of the training set. Nevertheless, the applied methods use cross validation methodology and so out of sample or holding sample can be used to assess accuracy. 

Before going further, let's call the needed libraries to be used in the analysis

```{r, warning= FALSE, message=FALSE}
library(caret)
library(ggplot2)
library(gridExtra)
library(polycor)
library(mlbench)
library(knitr)
```

### Data cleansing
After analysing the data we found some important features:

1.- The first 7 columns are not measures from the accelerometers, and they are useless as predictors

```{r, echo=FALSE}
str(training[,1:7])
training <- training[,-c(1:7)]
testing<- testing[,-c(1:7)]
```

2.- There is a lot of "NA" values in quite a few variables. We propose the following code to asses the number of "NA" in each column:

```{r}
HowmanyNa<-sapply(training, function(x) round(mean(is.na(x)),3))
table(HowmanyNa)
```

67 out of the 153 remaining variables (after dropping the first 7 initial columns) have more than 97% rows with "NA" values. Imputing NA would not be a good idea with such a huge proportion of missing values. We decided to eliminate those columns before the analysis.

```{r}
training <- training[,HowmanyNa<0.97]
testing <- testing[,HowmanyNa<0.97]
dim(training); dim(testing)
```

Now we have 86 valid columns left.

3.- A final decisions is to eliminate variables with low variance, so as to reduce dimension. We applied the R function "nearZeroVar" to the training data and drop out more than 30 additional columns.

```{r}
Lowvar <- nearZeroVar(training)
training_clear <- training[,-Lowvar]
testing_clear <- testing[,-Lowvar]
dim(training_clear); dim(testing_clear)
```

Now the training set has 53 variables left

## Initial assessment

Before applying machine learning algorithms let's see if we can rapidly assess the most relevant variables to explain **"classe"**. A way to see that, is the use of correlation between **"classe"** and each one of the 52 predictors. **"classe"** is a categorical variable and traditional Pearson correlation would not work. We applied the R function "hetcor" instead. 

```{r, echo=FALSE, warning= FALSE, message=FALSE}
correl<- hetcor(training_clear)$correlations[,53]
## cleaning NA values and correl=1("classe"" itself)
clean1<-subset(correl, !is.na(correl))
clean2<-subset(correl, !(correl==1))
correlClean <- as.data.frame(abs(clean2))
```

As we are only interested in the importance of the predictor, we use the absolute value of the correlations and propose a first plot:

```{r, echo=FALSE}
g1<-ggplot(correlClean, aes(x=reorder(rownames(correlClean), abs(clean2)), y=abs(clean2)))+
        geom_bar(stat="identity",color="white", fill="blue", size=0.5)+
        coord_flip()+
        labs(title="'Classe' correlation with predictors", x="Predictors", y= "Correlation")
g1
```

"pitch_forearm"seems to be the more explaining covariate, followed by "magnet_belt_y" and "magnet_arm_x". We will see below if they are the most influencing variables.

## Machine Learning methods

From now on we will apply some Machine Learning predicting methods. They will allow us to assess the importance of the predictors (to compare with the picture above) and, most important, they will be used to predict "classe" in the test set. In all the methods we will assess out of sample accuracy.

### preparing parallel running
All methods that we have tried, use the package **"caret"**. We have applied instructions given by **Len Greski** so as to improve the performance of the **train** function in **caret**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
if(!require(foreach)) install.packages("foreach")
if(!require(doParallel)) install.packages("doParallel")
library(parallel)
library(doParallel) 
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

# Configure trainControl object
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)
```

### 1^st^ Model: Linear Discriminant Analysis
Before running this first method, let`s define "" (predictor's matrix) and "y" (outcome vector) to improve performance:

```{r}
x <- training_clear[,-53]
y <- training_clear[,53]
```

Now we will use the **"train"**" function with **method ="lda"** to apply linear discriminant analysis. We use confusion matrix to asses the out of sample accuracy in the training set

```{r}
set.seed(9530)
Modellda <- train(x,y,method="lda", 
                    trControl = fitControl)
confusionMatrix.train(Modellda)
```
The confusion matrix gives a rather poor result of 70% accuracy. Maybe this is not the best model. Nevertheless, predictions for the test set are stored in the **predictlda** variable.

```{r echo=FALSE}
predictlda <- predict(Modellda, newdata=testing_clear)
kable(predictlda, col.names= c("lda_predictions"), row.names=TRUE)
```


### 2^nd^ Model: Random Forest

Our second try is a random forest model with the following code:

```{r, cache=TRUE}
set.seed(2351)
Modelrf <- train(x,y,method="rf", 
                  trControl = fitControl)
confusionMatrix.train(Modelrf)
```
Results form the confusion matrix are much more promising. We now use the model to asses the importance of the predictors and compare with the correlation study made above.

```{r, echo=FALSE}
Importancerf <-varImp(Modelrf)$importance
kable(Importancerf,col.names = c("Importance"), row.names = TRUE)
```

The following picture illustrates the comparison between importance and correlation

```{r, echo=FALSE, fig.align="center", fig.width=8}
g2 <- ggplot(Importancerf, aes(x=reorder(rownames(Importancerf), Overall), y=Overall))+
        geom_bar(stat="identity",color="white", fill="red", size=0.5)+
        coord_flip()+
        labs(title="Random Forest importance", x="Predictors", y= "Importance")
grid.arrange(g1,g2, nrow=1)
```

And, finally, the predictions for the test set

```{r, echo=FALSE}
predictrf <- predict(Modelrf, newdata=testing_clear)
kable(predictrf, col.names= c("rf_predictions"), row.names=TRUE)
```

### 3^rd^ Model: Generalized Boosted Models
Our final try is a "generalized boosted model" using the following code:

```{r}
set.seed(3541)
Modelgbm <- train(x,y,method="gbm", 
                 trControl = fitControl, verbose=FALSE)
confusionMatrix.train(Modelgbm)
```

Confusion Matrix shows lower accuracy than in the random forest case, but the model works better that "lda". A plot can illustrate the relative importance of the predictors under this model compared with the Random Forest case

```{r, fig.align='center', echo=FALSE}
influence<-summary(Modelgbm, plotit=FALSE)
g3 <- ggplot(influence, aes(x=reorder(var, rel.inf), y=rel.inf))+
        geom_bar(stat="identity",color="white", fill="green", size=0.5)+
        coord_flip()+
        labs(title="GBM Model importance", x="Predictors", y= "Importance")
grid.arrange(g2,g3, nrow=1)
```

Most of the variables have same relative importance in the two models. We now make predictions for the test set, and compare them with predictions from the rf model

```{r, echo=FALSE}
predictgbm <- predict(Modelgbm, newdata=testing_clear)
kable(predictgbm, col.names= c("rgbm_predictions"), row.names=TRUE)
```

Although the "gbm" model is less accurate than "rf" model for the out of sample data, predictions for the test set are exactly         the same, as shown in the table below:

```{r compare_predictions}
table(predictgbm, predictrf)
```

```{r, echo = FALSE}
# De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
```

## Summary and conclussions

Three model have been proposed to predict the **"classe"** variable in the test set. Out of sample accuracy has been assessed and the Random Forest ("rf") model seems to be the most accurate. The table below compares accuracy for the three models

###Accuracy of the models
```{r, echo=FALSE}
predTable <- data.frame(predlda=predictlda, predrf=predictrf, predgbm=predictgbm)
accuracy <- data.frame(lda=0.7, rf=0.993, gbm=0.963)
accuracy
```

Next table depicts predictions for teh three proposed models:

```{r, echo=FALSE}
kable(predTable, col.names = c("lda", "rf", "gbm"), row.names = TRUE)
```

The most accurate method: Random Forest (rf) will be finally used to predict test data.
